# Glossary terms from module 1

## Terms and definitions for Course 4, Module 1

- **Accuracy**: The degree to which the data conforms to the actual entity being measured or described
- **Completeness**: The degree to which the data contains all desired components or measures
- **Confidence interval**:  A range of values that conveys how likely a statistical estimate reflects the population
- **Confidence level**: The probability that a sample size accurately reflects the greater population
- **Consistency**: The degree to which data is repeatable from different points of entry or collection
- **Cross-field validation**: A process that ensures certain conditions for multiple data fields are satisfied
- **Data constraints**: The criteria that determine whether a piece of a data is clean and valid
- **Data integrity**: The accuracy, completeness, consistency, and trustworthiness of data throughout its life cycle
- **Data manipulation**: The process of changing data to make it more organized and easier to read
- **Data range**: Numerical values that fall between predefined maximum and minimum values
- **Data replication**: The process of storing data in multiple locations
- **DATEDIF**: A spreadsheet function that calculates the number of days, months, or years between two dates
- **Estimated response rate**: The average number of people who typically complete a survey
- **Hypothesis testing**: A process to determine if a survey or experiment has meaningful results
- **Mandatory**: A data value that cannot be left blank or empty
- **Margin of error**: The maximum amount that the sample results are expected to differ from those of the actual population
- **Random sampling**: A way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen
- **Regular expression (RegEx)**: A rule that says the values in a table must match a prescribed pattern
